{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DLRM Training for Book Recommendation System\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1: Import Libraries and Setup Environment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_27202/3482653582.py:21: DeprecationWarning: `torch.distributed._sharded_tensor` will be deprecated, use `torch.distributed._shard.sharded_tensor` instead\n",
      "  from torch.distributed._sharded_tensor import ShardedTensor\n",
      "/home/mr-behdadi/PROJECT/ICE/venv/lib/python3.13/site-packages/torch/distributed/algorithms/ddp_comm_hooks/__init__.py:54: FutureWarning: functools.partial will be a method descriptor in future Python versions; wrap it in enum.member() if you want to preserve the old behavior\n",
      "  ALLREDUCE = partial(_ddp_comm_hook_wrapper, comm_hook=default.allreduce_hook)\n",
      "/home/mr-behdadi/PROJECT/ICE/venv/lib/python3.13/site-packages/torch/distributed/algorithms/ddp_comm_hooks/__init__.py:55: FutureWarning: functools.partial will be a method descriptor in future Python versions; wrap it in enum.member() if you want to preserve the old behavior\n",
      "  FP16_COMPRESS = partial(\n",
      "/home/mr-behdadi/PROJECT/ICE/venv/lib/python3.13/site-packages/torch/distributed/algorithms/ddp_comm_hooks/__init__.py:58: FutureWarning: functools.partial will be a method descriptor in future Python versions; wrap it in enum.member() if you want to preserve the old behavior\n",
      "  BF16_COMPRESS = partial(\n",
      "/home/mr-behdadi/PROJECT/ICE/venv/lib/python3.13/site-packages/torch/distributed/algorithms/ddp_comm_hooks/__init__.py:61: FutureWarning: functools.partial will be a method descriptor in future Python versions; wrap it in enum.member() if you want to preserve the old behavior\n",
      "  QUANTIZE_PER_TENSOR = partial(\n",
      "/home/mr-behdadi/PROJECT/ICE/venv/lib/python3.13/site-packages/torch/distributed/algorithms/ddp_comm_hooks/__init__.py:64: FutureWarning: functools.partial will be a method descriptor in future Python versions; wrap it in enum.member() if you want to preserve the old behavior\n",
      "  QUANTIZE_PER_CHANNEL = partial(\n",
      "/home/mr-behdadi/PROJECT/ICE/venv/lib/python3.13/site-packages/torch/distributed/algorithms/ddp_comm_hooks/__init__.py:67: FutureWarning: functools.partial will be a method descriptor in future Python versions; wrap it in enum.member() if you want to preserve the old behavior\n",
      "  POWER_SGD = partial(\n",
      "/home/mr-behdadi/PROJECT/ICE/venv/lib/python3.13/site-packages/torch/distributed/algorithms/ddp_comm_hooks/__init__.py:74: FutureWarning: functools.partial will be a method descriptor in future Python versions; wrap it in enum.member() if you want to preserve the old behavior\n",
      "  POWER_SGD_RANK2 = partial(\n",
      "/home/mr-behdadi/PROJECT/ICE/venv/lib/python3.13/site-packages/torch/distributed/algorithms/ddp_comm_hooks/__init__.py:80: FutureWarning: functools.partial will be a method descriptor in future Python versions; wrap it in enum.member() if you want to preserve the old behavior\n",
      "  BATCHED_POWER_SGD = partial(\n",
      "/home/mr-behdadi/PROJECT/ICE/venv/lib/python3.13/site-packages/torch/distributed/algorithms/ddp_comm_hooks/__init__.py:85: FutureWarning: functools.partial will be a method descriptor in future Python versions; wrap it in enum.member() if you want to preserve the old behavior\n",
      "  BATCHED_POWER_SGD_RANK2 = partial(\n",
      "/home/mr-behdadi/PROJECT/ICE/venv/lib/python3.13/site-packages/torch/distributed/algorithms/ddp_comm_hooks/__init__.py:90: FutureWarning: functools.partial will be a method descriptor in future Python versions; wrap it in enum.member() if you want to preserve the old behavior\n",
      "  NOOP = partial(\n",
      "/home/mr-behdadi/PROJECT/ICE/venv/lib/python3.13/site-packages/torch/cuda/__init__.py:182: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at /pytorch/c10/cuda/CUDAFunctions.cpp:109.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n",
      "/home/mr-behdadi/PROJECT/ICE/venv/lib/python3.13/site-packages/torchrec/sparse/jagged_tensor.py:588: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _register_pytree_node(JaggedTensor, _jt_flatten, _jt_unflatten)\n",
      "/home/mr-behdadi/PROJECT/ICE/venv/lib/python3.13/site-packages/torchrec/sparse/jagged_tensor.py:1993: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _register_pytree_node(\n",
      "/usr/lib/python3.13/warnings.py:637: FutureWarning: `to_str_fn` and `maybe_from_str_fn` is deprecated. Please use `to_dumpable_context` and `from_dumpable_context` instead.\n",
      "  return arg(*args, **kwargs)\n",
      "/home/mr-behdadi/PROJECT/ICE/venv/lib/python3.13/site-packages/torchrec/sparse/jagged_tensor.py:2199: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _register_pytree_node(KeyedTensor, _kt_flatten, _kt_unflatten)\n"
     ]
    }
   ],
   "source": [
    "import contextlib\n",
    "import tempfile\n",
    "from typing import Generator\n",
    "import csv\n",
    "import os\n",
    "import random\n",
    "import pickle\n",
    "import sys\n",
    "import itertools\n",
    "from functools import partial\n",
    "from collections import defaultdict\n",
    "from dataclasses import dataclass, field, asdict\n",
    "from typing import List, Optional, Iterable, cast\n",
    "\n",
    "# PyTorch and distributed training\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "from torch.distributed._sharded_tensor import ShardedTensor\n",
    "\n",
    "# TorchRec for DLRM\n",
    "from torchrec import EmbeddingBagCollection\n",
    "from torchrec.distributed import TrainPipelineSparseDist\n",
    "from torchrec.distributed.comm import get_local_size\n",
    "from torchrec.distributed.model_parallel import (\n",
    "    DistributedModelParallel,\n",
    "    get_default_sharders,\n",
    ")\n",
    "from torchrec.distributed.planner import EmbeddingShardingPlanner, Topology\n",
    "from torchrec.distributed.planner.storage_reservations import (\n",
    "    HeuristicalStorageReservation,\n",
    ")\n",
    "from torchrec.models.dlrm import DLRM, DLRM_DCN, DLRM_Projection, DLRMTrain\n",
    "from torchrec.optim.apply_optimizer_in_backward import apply_optimizer_in_backward\n",
    "from torchrec.modules.embedding_configs import EmbeddingBagConfig\n",
    "from torchrec.optim.keyed import CombinedOptimizer, KeyedOptimizerWrapper\n",
    "from torchrec.optim.optimizers import in_backward_optimizer_filter\n",
    "from torchrec.datasets.utils import Batch\n",
    "from torchrec.sparse.jagged_tensor import KeyedJaggedTensor\n",
    "\n",
    "# Streaming data\n",
    "from streaming import StreamingDataset, StreamingDataLoader\n",
    "from streaming.base import MDSWriter\n",
    "\n",
    "# Metrics and utilities\n",
    "import torchmetrics as metrics\n",
    "from tqdm import tqdm\n",
    "from pyre_extensions import none_throws\n",
    "import mlflow\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2: Setup Environment Variables for Distributed Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Setup environment for single GPU/CPU training\n",
    "os.environ[\"RANK\"] = \"0\"\n",
    "os.environ[\"LOCAL_RANK\"] = \"0\" \n",
    "os.environ[\"WORLD_SIZE\"] = \"1\"\n",
    "os.environ[\"MASTER_ADDR\"] = \"localhost\"\n",
    "os.environ[\"MASTER_PORT\"] = \"29500\"\n",
    "\n",
    "# Setup device and backend\n",
    "device = torch.device(\"cpu\")  # Using CPU for compatibility\n",
    "backend = \"gloo\"\n",
    "\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3: Load Preprocessing Information\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading book data preprocessing info...\n",
      "‚úÖ Preprocessing info loaded successfully\n",
      "Dense columns (6): ['age_normalized', 'year_normalized', 'user_activity', 'book_popularity', 'user_avg_rating', 'book_avg_rating']\n",
      "Categorical columns (7): ['user_id_encoded', 'book_id_encoded', 'publisher_encoded', 'country_encoded', 'age_group', 'decade_encoded', 'rating_level']\n",
      "Embedding counts: {'user_id_encoded': 2578, 'book_id_encoded': 4313, 'publisher_encoded': 315, 'country_encoded': 45, 'age_group': 6, 'decade_encoded': 7, 'rating_level': 4}\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading book data preprocessing info...\")\n",
    "\n",
    "with open('/home/mr-behdadi/PROJECT/ICE/book_dlrm_preprocessing.pkl', 'rb') as f:\n",
    "        preprocessing_info = pickle.load(f)\n",
    "    \n",
    "dense_cols = preprocessing_info['dense_cols']\n",
    "cat_cols = preprocessing_info['cat_cols'] \n",
    "emb_counts = preprocessing_info['emb_counts']\n",
    "    \n",
    "print(\"‚úÖ Preprocessing info loaded successfully\")\n",
    "print(f\"Dense columns ({len(dense_cols)}): {dense_cols}\")\n",
    "print(f\"Categorical columns ({len(cat_cols)}): {cat_cols}\")\n",
    "print(f\"Embedding counts: {dict(zip(cat_cols, emb_counts))}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4: Define Training Arguments Dataclass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Training arguments dataclass defined\n"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class BookDLRMArgs:\n",
    "    \"\"\"Training arguments for book recommendation DLRM\"\"\"\n",
    "    epochs: int = 3\n",
    "    embedding_dim: int = 64\n",
    "    dense_arch_layer_sizes: list = field(default_factory=lambda: [256, 128, 64])  \n",
    "    over_arch_layer_sizes: list = field(default_factory=lambda: [512, 256, 128, 1])\n",
    "    learning_rate: float = 0.01\n",
    "    eps: float = 1e-8\n",
    "    batch_size: int = 512\n",
    "    print_sharding_plan: bool = True\n",
    "    print_lr: bool = False\n",
    "    lr_warmup_steps: int = 0  # Set to 0 to avoid scheduler issues\n",
    "    lr_decay_start: int = 0\n",
    "    lr_decay_steps: int = 0\n",
    "    validation_freq: int = None\n",
    "    limit_train_batches: int = None\n",
    "    limit_val_batches: int = None\n",
    "    limit_test_batches: int = None\n",
    "\n",
    "@dataclass\n",
    "class TrainValTestResults:\n",
    "    \"\"\"Results storage for training\"\"\"\n",
    "    val_aurocs: List[float] = field(default_factory=list)\n",
    "    test_auroc: Optional[float] = None\n",
    "\n",
    "print(\"‚úÖ Training arguments dataclass defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5: Define Batch Transformation Function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Batch transformation function defined\n"
     ]
    }
   ],
   "source": [
    "def transform_to_torchrec_batch(batch, num_embeddings_per_feature: Optional[List[int]] = None) -> Batch:\n",
    "    \"\"\"Transform batch to TorchRec format for book recommendation data\"\"\"\n",
    "    # Dense features\n",
    "    cat_list = []\n",
    "    for col_name in dense_cols:\n",
    "        val = torch.tensor(batch[col_name], dtype=torch.float32)\n",
    "        cat_list.append(val.unsqueeze(0).T)\n",
    "    dense_features = torch.cat(cat_list, dim=1)\n",
    "\n",
    "    # Sparse features\n",
    "    kjt_values: List[int] = []\n",
    "    kjt_lengths: List[int] = []\n",
    "    for col_idx, col_name in enumerate(cat_cols):\n",
    "        values = batch[col_name]\n",
    "        for value in values:\n",
    "            if value is not None and value >= 0:\n",
    "                kjt_values.append(\n",
    "                    int(value) % num_embeddings_per_feature[col_idx]\n",
    "                )\n",
    "                kjt_lengths.append(1)\n",
    "            else:\n",
    "                kjt_lengths.append(0)\n",
    "\n",
    "    sparse_features = KeyedJaggedTensor.from_lengths_sync(\n",
    "        cat_cols,\n",
    "        torch.tensor(kjt_values),\n",
    "        torch.tensor(kjt_lengths, dtype=torch.int32),\n",
    "    )\n",
    "    \n",
    "    # Labels\n",
    "    labels = torch.tensor(batch[\"label\"], dtype=torch.int32)\n",
    "    assert isinstance(labels, torch.Tensor)\n",
    "\n",
    "    return Batch(\n",
    "        dense_features=dense_features,\n",
    "        sparse_features=sparse_features,\n",
    "        labels=labels,\n",
    "    )\n",
    "\n",
    "print(\"‚úÖ Batch transformation function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6: Create Transform Partial Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Transform partial function created\n"
     ]
    }
   ],
   "source": [
    "# Create partial function with embedding counts\n",
    "if emb_counts:\n",
    "    transform_partial = partial(transform_to_torchrec_batch, num_embeddings_per_feature=emb_counts)\n",
    "    print(\"‚úÖ Transform partial function created\")\n",
    "else:\n",
    "    print(\"‚ùå Cannot create transform function - missing embedding counts\")\n",
    "    transform_partial = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7: Define DataLoader Creation Function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ DataLoader creation function defined\n"
     ]
    }
   ],
   "source": [
    "def get_dataloader_with_mosaic(path, batch_size, label):\n",
    "    \"\"\"Get DataLoader for book recommendation data\"\"\"\n",
    "    print(f\"Getting {label} data from {path}\")\n",
    "    try:\n",
    "        dataset = StreamingDataset(local=path, shuffle=True, batch_size=batch_size)\n",
    "        dataloader = StreamingDataLoader(dataset, batch_size=batch_size)\n",
    "        print(f\"‚úÖ {label} dataloader created successfully\")\n",
    "        return dataloader\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error creating {label} dataloader: {e}\")\n",
    "        return None\n",
    "\n",
    "print(\"‚úÖ DataLoader creation function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8: Define Learning Rate Scheduler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Learning rate scheduler defined\n"
     ]
    }
   ],
   "source": [
    "class LRPolicyScheduler(_LRScheduler):\n",
    "    \"\"\"Learning rate scheduler with warmup and decay\"\"\"\n",
    "    def __init__(self, optimizer, num_warmup_steps, decay_start_step, num_decay_steps):\n",
    "        self.num_warmup_steps = num_warmup_steps\n",
    "        self.decay_start_step = decay_start_step\n",
    "        self.decay_end_step = decay_start_step + num_decay_steps\n",
    "        self.num_decay_steps = num_decay_steps\n",
    "\n",
    "        if self.decay_start_step < self.num_warmup_steps:\n",
    "            print(\"Warning: Learning rate warmup must finish before the decay starts\")\n",
    "\n",
    "        super(LRPolicyScheduler, self).__init__(optimizer)\n",
    "\n",
    "    def get_lr(self):\n",
    "        step_count = self._step_count\n",
    "        if step_count < self.num_warmup_steps:\n",
    "            # warmup\n",
    "            scale = 1.0 - (self.num_warmup_steps - step_count) / self.num_warmup_steps\n",
    "            lr = [base_lr * scale for base_lr in self.base_lrs]\n",
    "            self.last_lr = lr\n",
    "        elif self.decay_start_step <= step_count and step_count < self.decay_end_step:\n",
    "            # decay\n",
    "            decayed_steps = step_count - self.decay_start_step\n",
    "            scale = ((self.num_decay_steps - decayed_steps) / self.num_decay_steps) ** 2\n",
    "            min_lr = 0.0000001\n",
    "            lr = [max(min_lr, base_lr * scale) for base_lr in self.base_lrs]\n",
    "            self.last_lr = lr\n",
    "        else:\n",
    "            if self.num_decay_steps > 0:\n",
    "                lr = self.last_lr\n",
    "            else:\n",
    "                lr = self.base_lrs\n",
    "        return lr\n",
    "\n",
    "print(\"‚úÖ Learning rate scheduler defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9: Define Utility Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Utility functions defined\n"
     ]
    }
   ],
   "source": [
    "def batched(it, n):\n",
    "    \"\"\"Helper function for batching\"\"\"\n",
    "    assert n >= 1\n",
    "    for x in it:\n",
    "        yield itertools.chain((x,), itertools.islice(it, n - 1))\n",
    "\n",
    "def get_relevant_fields(args, dense_cols, cat_cols, emb_counts):\n",
    "    \"\"\"Get relevant fields for MLflow logging\"\"\"\n",
    "    fields_to_save = [\"epochs\", \"embedding_dim\", \"dense_arch_layer_sizes\", \n",
    "                     \"over_arch_layer_sizes\", \"learning_rate\", \"eps\", \"batch_size\"]\n",
    "    result = {key: getattr(args, key) for key in fields_to_save}\n",
    "    result[\"dense_cols\"] = dense_cols\n",
    "    result[\"cat_cols\"] = cat_cols\n",
    "    result[\"emb_counts\"] = emb_counts\n",
    "    return result\n",
    "\n",
    "print(\"‚úÖ Utility functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10: Define Training Function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Training function defined\n"
     ]
    }
   ],
   "source": [
    "def train(\n",
    "    pipeline: TrainPipelineSparseDist,\n",
    "    train_dataloader: DataLoader,\n",
    "    val_dataloader: DataLoader,\n",
    "    epoch: int,\n",
    "    lr_scheduler,\n",
    "    print_lr: bool,\n",
    "    validation_freq: Optional[int],\n",
    "    limit_train_batches: Optional[int],\n",
    "    limit_val_batches: Optional[int]) -> None:\n",
    "    \"\"\"Train model for 1 epoch\"\"\"\n",
    "    \n",
    "    pipeline._model.train()\n",
    "    iterator = itertools.islice(iter(train_dataloader), limit_train_batches)\n",
    "    \n",
    "    is_rank_zero = dist.get_rank() == 0\n",
    "    if is_rank_zero:\n",
    "        pbar = tqdm(\n",
    "            iter(int, 1),\n",
    "            desc=f\"Epoch {epoch + 1}\",\n",
    "            total=len(train_dataloader),\n",
    "            disable=False,\n",
    "        )\n",
    "\n",
    "    start_it = 0\n",
    "    n = validation_freq if validation_freq else len(train_dataloader)\n",
    "    for batched_iterator in batched(iterator, n):\n",
    "        for it in itertools.count(start_it):\n",
    "            try:\n",
    "                if is_rank_zero and print_lr:\n",
    "                    for i, g in enumerate(pipeline._optimizer.param_groups):\n",
    "                        print(f\"lr: {it} {i} {g['lr']:.6f}\")\n",
    "                pipeline.progress(map(transform_partial, batched_iterator))\n",
    "                lr_scheduler.step()\n",
    "                if is_rank_zero:\n",
    "                    pbar.update(1)\n",
    "            except StopIteration:\n",
    "                if is_rank_zero:\n",
    "                    print(f\"Completed {it} training iterations\")\n",
    "                start_it = it\n",
    "                break\n",
    "\n",
    "        if validation_freq and start_it % validation_freq == 0:\n",
    "            evaluate(limit_val_batches, pipeline, val_dataloader, \"val\")\n",
    "            pipeline._model.train()\n",
    "\n",
    "print(\"‚úÖ Training function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11: Define Evaluation Function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Evaluation function defined\n"
     ]
    }
   ],
   "source": [
    "def evaluate(\n",
    "    limit_batches: Optional[int],\n",
    "    pipeline: TrainPipelineSparseDist,\n",
    "    eval_dataloader: DataLoader,\n",
    "    stage: str) -> float:\n",
    "    \"\"\"Evaluate model and compute AUROC\"\"\"\n",
    "    \n",
    "    pipeline._model.eval()\n",
    "    device = pipeline._device\n",
    "\n",
    "    iterator = itertools.islice(iter(eval_dataloader), limit_batches)\n",
    "    auroc = metrics.AUROC(task=\"binary\").to(device)\n",
    "\n",
    "    is_rank_zero = dist.get_rank() == 0\n",
    "    if is_rank_zero:\n",
    "        pbar = tqdm(\n",
    "            iter(int, 1),\n",
    "            desc=f\"Evaluating {stage} set\",\n",
    "            total=len(eval_dataloader),\n",
    "            disable=False,\n",
    "        )\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        while True:\n",
    "            try:\n",
    "                _loss, logits, labels = pipeline.progress(map(transform_partial, iterator))\n",
    "                preds = torch.sigmoid(logits)\n",
    "                auroc(preds, labels)\n",
    "                if is_rank_zero:\n",
    "                    pbar.update(1)\n",
    "            except StopIteration:\n",
    "                break\n",
    "\n",
    "    auroc_result = auroc.compute().item()\n",
    "    num_samples = torch.tensor(sum(map(len, auroc.target)), device=device)\n",
    "    dist.reduce(num_samples, 0, op=dist.ReduceOp.SUM)\n",
    "\n",
    "    if is_rank_zero:\n",
    "        print(f\"AUROC over {stage} set: {auroc_result:.4f}\")\n",
    "        print(f\"Number of {stage} samples: {num_samples}\")\n",
    "    return auroc_result\n",
    "\n",
    "print(\"‚úÖ Evaluation function defined\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12: Define Complete Training Loop Function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Complete training loop function defined\n"
     ]
    }
   ],
   "source": [
    "def train_val_test(args, model, optimizer, device, train_dataloader, val_dataloader, test_dataloader, lr_scheduler) -> TrainValTestResults:\n",
    "    \"\"\"Complete training loop\"\"\"\n",
    "    \n",
    "    results = TrainValTestResults()\n",
    "    pipeline = TrainPipelineSparseDist(model, optimizer, device, execute_all_batches=True)\n",
    "    \n",
    "    # Initial validation\n",
    "    print(\"Running initial validation...\")\n",
    "    val_auroc = evaluate(args.limit_val_batches, pipeline, val_dataloader, \"val\")\n",
    "    results.val_aurocs.append(val_auroc)\n",
    "    if int(os.environ[\"RANK\"]) == 0:\n",
    "        mlflow.log_metric('val_auroc', val_auroc, step=0)\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(args.epochs):\n",
    "        print(f\"\\n=== Epoch {epoch + 1}/{args.epochs} ===\")\n",
    "        \n",
    "        train(\n",
    "            pipeline,\n",
    "            train_dataloader,\n",
    "            val_dataloader,\n",
    "            epoch,\n",
    "            lr_scheduler,\n",
    "            args.print_lr,\n",
    "            args.validation_freq,\n",
    "            args.limit_train_batches,\n",
    "            args.limit_val_batches,\n",
    "        )\n",
    "\n",
    "        # Validate after each epoch\n",
    "        val_auroc = evaluate(args.limit_val_batches, pipeline, val_dataloader, \"val\")\n",
    "        results.val_aurocs.append(val_auroc)\n",
    "        if int(os.environ[\"RANK\"]) == 0:\n",
    "            mlflow.log_metric('val_auroc', val_auroc, step=epoch + 1)\n",
    "            \n",
    "            # Save model state\n",
    "            model_path = f\"dlrm_book_model_epoch_{epoch}.pth\"\n",
    "            torch.save(pipeline._model.state_dict(), model_path)\n",
    "            print(f\"Model saved: {model_path}\")\n",
    "\n",
    "    # Final test evaluation\n",
    "    print(\"\\nRunning final test evaluation...\")\n",
    "    test_auroc = evaluate(args.limit_test_batches, pipeline, test_dataloader, \"test\")\n",
    "    results.test_auroc = test_auroc\n",
    "    if int(os.environ[\"RANK\"]) == 0:\n",
    "        mlflow.log_metric('test_auroc', test_auroc)\n",
    "        \n",
    "        # Save final model\n",
    "        final_model_path = \"dlrm_book_model_final.pth\"\n",
    "        torch.save(pipeline._model.state_dict(), final_model_path)\n",
    "        print(f\"Final model saved: {final_model_path}\")\n",
    "        \n",
    "    return results\n",
    "\n",
    "print(\"‚úÖ Complete training loop function defined\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13: Initialize Training Arguments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Training arguments initialized\n",
      "Training configuration:\n",
      "  Epochs: 3\n",
      "  Batch size: 512\n",
      "  Embedding dim: 64\n",
      "  Learning rate: 0.01\n",
      "  Dense features: 6\n",
      "  Categorical features: 7\n"
     ]
    }
   ],
   "source": [
    "# Cell 13: Initialize Training Arguments\n",
    "# Create training arguments optimized for book recommendation\n",
    "args = BookDLRMArgs(\n",
    "    epochs=3,\n",
    "    embedding_dim=64,\n",
    "    batch_size=512,\n",
    "    learning_rate=0.01,\n",
    "    lr_warmup_steps=0,\n",
    "    dense_arch_layer_sizes=[256, 128, 64],\n",
    "    over_arch_layer_sizes=[512, 256, 128, 1]\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Training arguments initialized\")\n",
    "print(f\"Training configuration:\")\n",
    "print(f\"  Epochs: {args.epochs}\")\n",
    "print(f\"  Batch size: {args.batch_size}\")\n",
    "print(f\"  Embedding dim: {args.embedding_dim}\")\n",
    "print(f\"  Learning rate: {args.learning_rate}\")\n",
    "print(f\"  Dense features: {len(dense_cols)}\")\n",
    "print(f\"  Categorical features: {len(cat_cols)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14: Setup MLflow Experiment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/13 21:54:48 INFO mlflow.tracking.fluent: Experiment with name 'dlrm-book-recommendation-book_recommender' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ MLflow experiment set: dlrm-book-recommendation-book_recommender\n",
      "‚úÖ Parameters logged to MLflow\n"
     ]
    }
   ],
   "source": [
    "# Setup MLflow experiment\n",
    "username = \"book_recommender\"\n",
    "experiment_path = f'dlrm-book-recommendation-{username}'\n",
    "\n",
    "try:\n",
    "    experiment = mlflow.set_experiment(experiment_path)\n",
    "    print(f\"‚úÖ MLflow experiment set: {experiment_path}\")\n",
    "    \n",
    "    # Log parameters\n",
    "    param_dict = get_relevant_fields(args, dense_cols, cat_cols, emb_counts)\n",
    "    mlflow.log_params(param_dict)\n",
    "    print(\"‚úÖ Parameters logged to MLflow\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è MLflow setup warning: {e}\")\n",
    "    print(\"Training will continue without MLflow logging\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15: Initialize Distributed Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing distributed training...\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "‚úÖ Distributed training initialized\n",
      "Global rank: 0\n",
      "Local rank: 0\n",
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Initialize distributed training\n",
    "print(\"Initializing distributed training...\")\n",
    "try:\n",
    "    # Disable JIT for compatibility\n",
    "    torch.jit._state.disable()\n",
    "    \n",
    "    # Initialize process group\n",
    "    dist.init_process_group(backend=backend)\n",
    "    \n",
    "    global_rank = int(os.environ[\"RANK\"])\n",
    "    local_rank = int(os.environ[\"LOCAL_RANK\"])\n",
    "    \n",
    "    print(f\"‚úÖ Distributed training initialized\")\n",
    "    print(f\"Global rank: {global_rank}\")\n",
    "    print(f\"Local rank: {local_rank}\")\n",
    "    print(f\"Device: {device}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error initializing distributed training: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16: Load Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:streaming.base.dataset:Because `predownload` was not specified, it will default to 8*batch_size if batch_size is not None, otherwise 64. Prior to Streaming v0.7.0, `predownload` defaulted to max(batch_size, 256 * batch_size // num_canonical_nodes).\n",
      "WARNING:streaming.base.dataset:Because `predownload` was not specified, it will default to 8*batch_size if batch_size is not None, otherwise 64. Prior to Streaming v0.7.0, `predownload` defaulted to max(batch_size, 256 * batch_size // num_canonical_nodes).\n",
      "WARNING:streaming.base.dataset:Because `predownload` was not specified, it will default to 8*batch_size if batch_size is not None, otherwise 64. Prior to Streaming v0.7.0, `predownload` defaulted to max(batch_size, 256 * batch_size // num_canonical_nodes).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading book recommendation data...\n",
      "Getting train data from dlrm_book_data/mds_train\n",
      "‚úÖ train dataloader created successfully\n",
      "Getting val data from dlrm_book_data/mds_validation\n",
      "‚úÖ val dataloader created successfully\n",
      "Getting test data from dlrm_book_data/mds_test\n",
      "‚úÖ test dataloader created successfully\n",
      "‚úÖ All dataloaders created successfully\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "print(\"Loading book recommendation data...\")\n",
    "\n",
    "# Check if data directories exist\n",
    "data_paths = {\n",
    "    \"train\": \"dlrm_book_data/mds_train\",\n",
    "    \"validation\": \"dlrm_book_data/mds_validation\", \n",
    "    \"test\": \"dlrm_book_data/mds_test\"\n",
    "}\n",
    "\n",
    "missing_paths = [path for path in data_paths.values() if not os.path.exists(path)]\n",
    "\n",
    "if missing_paths:\n",
    "    print(\"‚ùå Missing data directories:\")\n",
    "    for path in missing_paths:\n",
    "        print(f\"   - {path}\")\n",
    "    print(\"Please run the preprocessing notebook first.\")\n",
    "    train_dataloader = val_dataloader = test_dataloader = None\n",
    "else:\n",
    "    train_dataloader = get_dataloader_with_mosaic(data_paths[\"train\"], args.batch_size, \"train\")\n",
    "    val_dataloader = get_dataloader_with_mosaic(data_paths[\"validation\"], args.batch_size, \"val\")\n",
    "    test_dataloader = get_dataloader_with_mosaic(data_paths[\"test\"], args.batch_size, \"test\")\n",
    "    \n",
    "    print(\"‚úÖ All dataloaders created successfully\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17: Create DLRM Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating embedding configurations...\n",
      "‚úÖ Created 7 embedding configurations\n",
      "Creating DLRM model...\n",
      "‚úÖ DLRM model created with 720,065 parameters\n"
     ]
    }
   ],
   "source": [
    "# Create embedding configs for book recommendation\n",
    "if cat_cols and emb_counts:\n",
    "    print(\"Creating embedding configurations...\")\n",
    "    eb_configs = [\n",
    "        EmbeddingBagConfig(\n",
    "            name=f\"t_{feature_name}\",\n",
    "            embedding_dim=args.embedding_dim,\n",
    "            num_embeddings=emb_counts[feature_idx],\n",
    "            feature_names=[feature_name],\n",
    "        )\n",
    "        for feature_idx, feature_name in enumerate(cat_cols)\n",
    "    ]\n",
    "    \n",
    "    print(f\"‚úÖ Created {len(eb_configs)} embedding configurations\")\n",
    "    \n",
    "    # Create DLRM model for book recommendation\n",
    "    print(\"Creating DLRM model...\")\n",
    "    dlrm_model = DLRM(\n",
    "        embedding_bag_collection=EmbeddingBagCollection(\n",
    "            tables=eb_configs, device=device\n",
    "        ),\n",
    "        dense_in_features=len(dense_cols),\n",
    "        dense_arch_layer_sizes=args.dense_arch_layer_sizes,\n",
    "        over_arch_layer_sizes=args.over_arch_layer_sizes,\n",
    "        dense_device=device,\n",
    "    )\n",
    "\n",
    "    train_model = DLRMTrain(dlrm_model)\n",
    "    model = train_model.to(device)\n",
    "    \n",
    "    num_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"‚úÖ DLRM model created with {num_params:,} parameters\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå Cannot create model - missing categorical columns or embedding counts\")\n",
    "    model = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 18: Setup Optimizer and Learning Rate Scheduler  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up optimizer...\n",
      "‚úÖ Optimizer and learning rate scheduler created\n",
      "‚úÖ Initial model saved: dlrm_book_model_initial.pth\n"
     ]
    }
   ],
   "source": [
    "if model is not None:\n",
    "    # Setup optimizer\n",
    "    print(\"Setting up optimizer...\")\n",
    "    optimizer = torch.optim.Adagrad(model.parameters(), lr=args.learning_rate, eps=args.eps)\n",
    "    \n",
    "    # Setup learning rate scheduler\n",
    "    lr_scheduler = LRPolicyScheduler(\n",
    "        optimizer, \n",
    "        args.lr_warmup_steps, \n",
    "        args.lr_decay_start, \n",
    "        args.lr_decay_steps\n",
    "    )\n",
    "    \n",
    "    print(\"‚úÖ Optimizer and learning rate scheduler created\")\n",
    "    \n",
    "    # Save initial model\n",
    "    try:\n",
    "        initial_model_path = \"dlrm_book_model_initial.pth\"\n",
    "        torch.save(model.state_dict(), initial_model_path)\n",
    "        print(f\"‚úÖ Initial model saved: {initial_model_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Warning: Could not save initial model: {e}\")\n",
    "        \n",
    "else:\n",
    "    print(\"‚ùå Cannot setup optimizer - model not created\")\n",
    "    optimizer = lr_scheduler = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 19: Run Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üöÄ Starting DLRM training for book recommendation...\n",
      "============================================================\n",
      "Running initial validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating val set:   0%|          | 0/77 [00:00<?, ?it/s]WARNING:streaming.base.dataset:Because `num_canonical_nodes` was not specified, and `shuffle_algo` is py1e, it will default to be equal to physical nodes. Prior to Streaming v0.7.0, `num_canonical_nodes` defaulted to 64 * physical nodes.\n",
      "WARNING:streaming.base.dataset:Because `shuffle_block_size` was not specified, it will default to max(4_000_000 // num_canonical_nodes, 1 << 18) if num_canonical_nodes is not None, otherwise 262144. Prior to Streaming v0.7.0, `shuffle_block_size` defaulted to 262144.\n",
      "/tmp/ipykernel_27202/2624063692.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  val = torch.tensor(batch[col_name], dtype=torch.float32)\n",
      "/tmp/ipykernel_27202/2624063692.py:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(batch[\"label\"], dtype=torch.int32)\n",
      "/tmp/ipykernel_27202/2624063692.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  val = torch.tensor(batch[col_name], dtype=torch.float32)\n",
      "/tmp/ipykernel_27202/2624063692.py:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(batch[\"label\"], dtype=torch.int32)\n",
      "Evaluating val set:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 73/77 [00:08<00:00, 16.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUROC over val set: 0.6262\n",
      "Number of val samples: 39389\n",
      "\n",
      "=== Epoch 1/3 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:streaming.base.dataset:Because `num_canonical_nodes` was not specified, and `shuffle_algo` is py1e, it will default to be equal to physical nodes. Prior to Streaming v0.7.0, `num_canonical_nodes` defaulted to 64 * physical nodes.\n",
      "WARNING:streaming.base.dataset:Because `shuffle_block_size` was not specified, it will default to max(4_000_000 // num_canonical_nodes, 1 << 18) if num_canonical_nodes is not None, otherwise 262144. Prior to Streaming v0.7.0, `shuffle_block_size` defaulted to 262144.\n",
      "Evaluating val set: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 77/77 [00:09<00:00,  8.14it/s]\n",
      "/tmp/ipykernel_27202/2624063692.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  val = torch.tensor(batch[col_name], dtype=torch.float32)\n",
      "/tmp/ipykernel_27202/2624063692.py:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(batch[\"label\"], dtype=torch.int32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed 270 training iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 270/270 [00:33<00:00,  8.10it/s]\n",
      "/tmp/ipykernel_27202/2624063692.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  val = torch.tensor(batch[col_name], dtype=torch.float32)\n",
      "/tmp/ipykernel_27202/2624063692.py:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(batch[\"label\"], dtype=torch.int32)\n",
      "Evaluating val set:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 75/77 [00:09<00:00, 15.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUROC over val set: 0.9990\n",
      "Number of val samples: 39389\n",
      "Model saved: dlrm_book_model_epoch_0.pth\n",
      "\n",
      "=== Epoch 2/3 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating val set: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 77/77 [00:09<00:00,  7.76it/s]\n",
      "/tmp/ipykernel_27202/2624063692.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  val = torch.tensor(batch[col_name], dtype=torch.float32)\n",
      "/tmp/ipykernel_27202/2624063692.py:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(batch[\"label\"], dtype=torch.int32)\n",
      "Epoch 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 270/270 [00:33<00:00,  7.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed 270 training iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating val set:   0%|          | 0/77 [00:00<?, ?it/s]/tmp/ipykernel_27202/2624063692.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  val = torch.tensor(batch[col_name], dtype=torch.float32)\n",
      "/tmp/ipykernel_27202/2624063692.py:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(batch[\"label\"], dtype=torch.int32)\n",
      "Evaluating val set: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 77/77 [00:08<00:00,  8.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUROC over val set: 0.9989\n",
      "Number of val samples: 39389\n",
      "Model saved: dlrm_book_model_epoch_1.pth\n",
      "\n",
      "=== Epoch 3/3 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:   0%|          | 0/270 [00:00<?, ?it/s]/tmp/ipykernel_27202/2624063692.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  val = torch.tensor(batch[col_name], dtype=torch.float32)\n",
      "/tmp/ipykernel_27202/2624063692.py:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(batch[\"label\"], dtype=torch.int32)\n",
      "Epoch 3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 270/270 [00:35<00:00,  7.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed 270 training iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating val set:   0%|          | 0/77 [00:00<?, ?it/s]/tmp/ipykernel_27202/2624063692.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  val = torch.tensor(batch[col_name], dtype=torch.float32)\n",
      "/tmp/ipykernel_27202/2624063692.py:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(batch[\"label\"], dtype=torch.int32)\n",
      "Evaluating val set: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 77/77 [00:08<00:00,  8.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUROC over val set: 0.9989\n",
      "Number of val samples: 39389\n",
      "Model saved: dlrm_book_model_epoch_2.pth\n",
      "\n",
      "Running final test evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating test set:   0%|          | 0/39 [00:00<?, ?it/s]WARNING:streaming.base.dataset:Because `num_canonical_nodes` was not specified, and `shuffle_algo` is py1e, it will default to be equal to physical nodes. Prior to Streaming v0.7.0, `num_canonical_nodes` defaulted to 64 * physical nodes.\n",
      "WARNING:streaming.base.dataset:Because `shuffle_block_size` was not specified, it will default to max(4_000_000 // num_canonical_nodes, 1 << 18) if num_canonical_nodes is not None, otherwise 262144. Prior to Streaming v0.7.0, `shuffle_block_size` defaulted to 262144.\n",
      "/tmp/ipykernel_27202/2624063692.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  val = torch.tensor(batch[col_name], dtype=torch.float32)\n",
      "/tmp/ipykernel_27202/2624063692.py:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(batch[\"label\"], dtype=torch.int32)\n",
      "Evaluating test set: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 39/39 [00:04<00:00,  8.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUROC over test set: 0.9989\n",
      "Number of test samples: 19714\n",
      "Final model saved: dlrm_book_model_final.pth\n",
      "\n",
      "üéâ Training completed successfully!\n",
      "üìä Final Results:\n",
      "   Final validation AUROC: 0.9989\n",
      "   Test AUROC: 0.9989\n",
      "   Best validation AUROC: 0.9990\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Start training\n",
    "if all([model, optimizer, train_dataloader, val_dataloader, test_dataloader, transform_partial]):\n",
    "    print(\"\\nüöÄ Starting DLRM training for book recommendation...\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    try:\n",
    "        results = train_val_test(\n",
    "            args,\n",
    "            model,\n",
    "            optimizer,\n",
    "            device,\n",
    "            train_dataloader,\n",
    "            val_dataloader,\n",
    "            test_dataloader,\n",
    "            lr_scheduler,\n",
    "        )\n",
    "        \n",
    "        print(\"\\nüéâ Training completed successfully!\")\n",
    "        print(f\"üìä Final Results:\")\n",
    "        print(f\"   Final validation AUROC: {results.val_aurocs[-1]:.4f}\")\n",
    "        print(f\"   Test AUROC: {results.test_auroc:.4f}\")\n",
    "        print(f\"   Best validation AUROC: {max(results.val_aurocs):.4f}\")\n",
    "        \n",
    "        training_completed = True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error during training: {e}\")\n",
    "        results = None\n",
    "        training_completed = False\n",
    "        \n",
    "else:\n",
    "    print(\"‚ùå Cannot start training - missing required components\")\n",
    "    missing_components = []\n",
    "    if model is None: missing_components.append(\"model\")\n",
    "    if optimizer is None: missing_components.append(\"optimizer\") \n",
    "    if train_dataloader is None: missing_components.append(\"train_dataloader\")\n",
    "    if val_dataloader is None: missing_components.append(\"val_dataloader\")\n",
    "    if test_dataloader is None: missing_components.append(\"test_dataloader\")\n",
    "    if transform_partial is None: missing_components.append(\"transform_partial\")\n",
    "    \n",
    "    print(f\"Missing: {', '.join(missing_components)}\")\n",
    "    results = None\n",
    "    training_completed = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cell 20: Save Training Results and Cleanup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Saving training results...\n",
      "‚úÖ Results saved to dlrm_book_training_results.pkl\n",
      "\n",
      "üìã Training Summary:\n",
      "   - Model parameters: 720,065\n",
      "   - Training epochs: 3\n",
      "   - Batch size: 512\n",
      "   - Learning rate: 0.01\n",
      "   - Embedding dimension: 64\n",
      "   - Dense features: 6\n",
      "   - Categorical features: 7\n",
      "   - Final test AUROC: 0.9989\n",
      "   - Best validation AUROC: 0.9990\n",
      "‚úÖ Distributed training cleanup completed\n",
      "\n",
      "============================================================\n",
      "üèÅ DLRM Book Recommendation Training Notebook Complete!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "if training_completed and results is not None:\n",
    "    print(\"üíæ Saving training results...\")\n",
    "    \n",
    "    try:\n",
    "        # Save comprehensive results\n",
    "        results_dict = {\n",
    "            'final_val_auroc': results.val_aurocs[-1],\n",
    "            'test_auroc': results.test_auroc,\n",
    "            'best_val_auroc': max(results.val_aurocs),\n",
    "            'val_aurocs_history': results.val_aurocs,\n",
    "            'args': asdict(args),\n",
    "            'preprocessing_info': {\n",
    "                'dense_cols': dense_cols,\n",
    "                'cat_cols': cat_cols,\n",
    "                'emb_counts': emb_counts\n",
    "            },\n",
    "            'model_info': {\n",
    "                'num_parameters': sum(p.numel() for p in model.parameters()),\n",
    "                'embedding_dim': args.embedding_dim,\n",
    "                'dense_features': len(dense_cols),\n",
    "                'categorical_features': len(cat_cols)\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        with open('dlrm_book_training_results.pkl', 'wb') as f:\n",
    "            pickle.dump(results_dict, f)\n",
    "        \n",
    "        print(\"‚úÖ Results saved to dlrm_book_training_results.pkl\")\n",
    "        \n",
    "        # Display training summary\n",
    "        print(f\"\\nüìã Training Summary:\")\n",
    "        print(f\"   - Model parameters: {results_dict['model_info']['num_parameters']:,}\")\n",
    "        print(f\"   - Training epochs: {args.epochs}\")\n",
    "        print(f\"   - Batch size: {args.batch_size}\")\n",
    "        print(f\"   - Learning rate: {args.learning_rate}\")\n",
    "        print(f\"   - Embedding dimension: {args.embedding_dim}\")\n",
    "        print(f\"   - Dense features: {len(dense_cols)}\")\n",
    "        print(f\"   - Categorical features: {len(cat_cols)}\")\n",
    "        print(f\"   - Final test AUROC: {results.test_auroc:.4f}\")\n",
    "        print(f\"   - Best validation AUROC: {max(results.val_aurocs):.4f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Warning: Could not save results: {e}\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ùå No results to save - training was not completed successfully\")\n",
    "\n",
    "# Cleanup distributed training\n",
    "try:\n",
    "    if dist.is_initialized():\n",
    "        dist.destroy_process_group()\n",
    "        print(\"‚úÖ Distributed training cleanup completed\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Warning during cleanup: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üèÅ DLRM Book Recommendation Training Notebook Complete!\")\n",
    "print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 21: Load and Test Saved Model (Optional)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ Testing saved model loading...\n",
      "‚úÖ Model state loaded successfully\n",
      "‚úÖ Training results loaded successfully\n",
      "Saved test AUROC: 0.9989\n",
      "Saved best validation AUROC: 0.9990\n",
      "Model info: {'num_parameters': 720065, 'embedding_dim': 64, 'dense_features': 6, 'categorical_features': 7}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Optional cell to test loading the saved model\n",
    "try:\n",
    "    print(\"üß™ Testing saved model loading...\")\n",
    "    \n",
    "    # Load model state\n",
    "    model_state = torch.load(\"dlrm_book_model_final.pth\", map_location=device)\n",
    "    print(\"‚úÖ Model state loaded successfully\")\n",
    "    \n",
    "    # Load training results\n",
    "    with open('dlrm_book_training_results.pkl', 'rb') as f:\n",
    "        saved_results = pickle.load(f)\n",
    "    \n",
    "    print(\"‚úÖ Training results loaded successfully\")\n",
    "    print(f\"Saved test AUROC: {saved_results['test_auroc']:.4f}\")\n",
    "    print(f\"Saved best validation AUROC: {saved_results['best_val_auroc']:.4f}\")\n",
    "    \n",
    "    # Display model information\n",
    "    print(f\"Model info: {saved_results['model_info']}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error testing saved model: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 22: Utility Functions for Model Inference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Inference utility functions defined\n",
      "\n",
      "üìö DLRM Book Recommendation Training Complete!\n",
      "üéØ Key accomplishments:\n",
      "   ‚úì Environment setup and library imports\n",
      "   ‚úì Data preprocessing info loaded\n",
      "   ‚úì DLRM model architecture created\n",
      "   ‚úì Training pipeline implemented\n",
      "   ‚úì Model evaluation with AUROC metrics\n",
      "   ‚úì Results saving and model checkpointing\n",
      "   ‚úì Utility functions for future inference\n",
      "\n",
      "üìÅ Generated files:\n",
      "   - dlrm_book_model_final.pth (final trained model)\n",
      "   - dlrm_book_training_results.pkl (training results)\n"
     ]
    }
   ],
   "source": [
    "def load_trained_model(model_path: str = \"dlrm_book_model_final.pth\"):\n",
    "    \"\"\"Load a trained DLRM model for inference\"\"\"\n",
    "    try:\n",
    "        # Load preprocessing info\n",
    "        with open('book_dlrm_preprocessing.pkl', 'rb') as f:\n",
    "            preprocessing_info = pickle.load(f)\n",
    "        \n",
    "        dense_cols = preprocessing_info['dense_cols']\n",
    "        cat_cols = preprocessing_info['cat_cols']\n",
    "        emb_counts = preprocessing_info['emb_counts']\n",
    "        \n",
    "        # Create model architecture (same as training)\n",
    "        eb_configs = [\n",
    "            EmbeddingBagConfig(\n",
    "                name=f\"t_{feature_name}\",\n",
    "                embedding_dim=64,  # Should match training\n",
    "                num_embeddings=emb_counts[feature_idx],\n",
    "                feature_names=[feature_name],\n",
    "            )\n",
    "            for feature_idx, feature_name in enumerate(cat_cols)\n",
    "        ]\n",
    "        \n",
    "        dlrm_model = DLRM(\n",
    "            embedding_bag_collection=EmbeddingBagCollection(\n",
    "                tables=eb_configs, device=device\n",
    "            ),\n",
    "            dense_in_features=len(dense_cols),\n",
    "            dense_arch_layer_sizes=[256, 128, 64],\n",
    "            over_arch_layer_sizes=[512, 256, 128, 1],\n",
    "            dense_device=device,\n",
    "        )\n",
    "        \n",
    "        train_model = DLRMTrain(dlrm_model)\n",
    "        model = train_model.to(device)\n",
    "        \n",
    "        # Load saved weights\n",
    "        model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "        model.eval()\n",
    "        \n",
    "        print(f\"‚úÖ Model loaded from {model_path}\")\n",
    "        return model, preprocessing_info\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading model: {e}\")\n",
    "        return None, None\n",
    "\n",
    "def predict_recommendation(model, preprocessing_info, user_features, book_features):\n",
    "    \"\"\"Make a recommendation prediction for a user-book pair\"\"\"\n",
    "    # This would need to be implemented based on your specific feature format\n",
    "    # Placeholder for inference logic\n",
    "    pass\n",
    "\n",
    "print(\"‚úÖ Inference utility functions defined\")\n",
    "\n",
    "# Final Summary\n",
    "print(f\"\\nüìö DLRM Book Recommendation Training Complete!\")\n",
    "print(f\"üéØ Key accomplishments:\")\n",
    "print(f\"   ‚úì Environment setup and library imports\")\n",
    "print(f\"   ‚úì Data preprocessing info loaded\")\n",
    "print(f\"   ‚úì DLRM model architecture created\")\n",
    "print(f\"   ‚úì Training pipeline implemented\")\n",
    "print(f\"   ‚úì Model evaluation with AUROC metrics\")\n",
    "print(f\"   ‚úì Results saving and model checkpointing\")\n",
    "print(f\"   ‚úì Utility functions for future inference\")\n",
    "\n",
    "print(f\"\\nüìÅ Generated files:\")\n",
    "print(f\"   - dlrm_book_model_final.pth (final trained model)\")\n",
    "print(f\"   - dlrm_book_training_results.pkl (training results)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 23: Hyperparameter Tuning Helper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Hyperparameter tuning helper functions defined\n",
      "Available configurations: config_small, config_medium, config_large\n"
     ]
    }
   ],
   "source": [
    "def create_training_config(\n",
    "    epochs: int = 5,\n",
    "    embedding_dim: int = 64,\n",
    "    batch_size: int = 512,\n",
    "    learning_rate: float = 0.01,\n",
    "    dense_layers: List[int] = None,\n",
    "    over_layers: List[int] = None\n",
    ") -> BookDLRMArgs:\n",
    "    \"\"\"Helper function to create different training configurations\"\"\"\n",
    "    \n",
    "    if dense_layers is None:\n",
    "        dense_layers = [256, 128, 64]\n",
    "    if over_layers is None:\n",
    "        over_layers = [512, 256, 128, 1]\n",
    "    \n",
    "    return BookDLRMArgs(\n",
    "        epochs=epochs,\n",
    "        embedding_dim=embedding_dim,\n",
    "        batch_size=batch_size,\n",
    "        learning_rate=learning_rate,\n",
    "        dense_arch_layer_sizes=dense_layers,\n",
    "        over_arch_layer_sizes=over_layers,\n",
    "        lr_warmup_steps=0\n",
    "    )\n",
    "\n",
    "# Example configurations\n",
    "config_small = create_training_config(\n",
    "    epochs=3,\n",
    "    embedding_dim=32,\n",
    "    batch_size=256,\n",
    "    dense_layers=[128, 64],\n",
    "    over_layers=[256, 128, 1]\n",
    ")\n",
    "\n",
    "config_medium = create_training_config(\n",
    "    epochs=5,\n",
    "    embedding_dim=64,\n",
    "    batch_size=512,\n",
    "    dense_layers=[256, 128, 64],\n",
    "    over_layers=[512, 256, 128, 1]\n",
    ")\n",
    "\n",
    "config_large = create_training_config(\n",
    "    epochs=10,\n",
    "    embedding_dim=128,\n",
    "    batch_size=1024,\n",
    "    dense_layers=[512, 256, 128],\n",
    "    over_layers=[1024, 512, 256, 1]\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Hyperparameter tuning helper functions defined\")\n",
    "print(\"Available configurations: config_small, config_medium, config_large\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cell 24: Model Performance Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Performance analysis function defined\n"
     ]
    }
   ],
   "source": [
    "def analyze_training_results(results_path: str = 'dlrm_book_training_results.pkl'):\n",
    "    \"\"\"Analyze training results and plot metrics\"\"\"\n",
    "    try:\n",
    "        with open(results_path, 'rb') as f:\n",
    "            results = pickle.load(f)\n",
    "        \n",
    "        print(\"üìä Training Results Analysis:\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        # Basic metrics\n",
    "        val_aurocs = results['val_aurocs_history']\n",
    "        test_auroc = results['test_auroc']\n",
    "        best_val_auroc = max(val_aurocs)\n",
    "        \n",
    "        print(f\"Final Test AUROC: {test_auroc:.4f}\")\n",
    "        print(f\"Best Validation AUROC: {best_val_auroc:.4f}\")\n",
    "        print(f\"Final Validation AUROC: {val_aurocs[-1]:.4f}\")\n",
    "        print(f\"Total Epochs: {len(val_aurocs)-1}\")\n",
    "        \n",
    "        # Model info\n",
    "        model_info = results.get('model_info', {})\n",
    "        print(f\"\\nModel Information:\")\n",
    "        print(f\"  Parameters: {model_info.get('num_parameters', 'N/A'):,}\")\n",
    "        print(f\"  Embedding Dimension: {model_info.get('embedding_dim', 'N/A')}\")\n",
    "        print(f\"  Dense Features: {model_info.get('dense_features', 'N/A')}\")\n",
    "        print(f\"  Categorical Features: {model_info.get('categorical_features', 'N/A')}\")\n",
    "        \n",
    "        # Training config\n",
    "        train_args = results.get('args', {})\n",
    "        print(f\"\\nTraining Configuration:\")\n",
    "        print(f\"  Batch Size: {train_args.get('batch_size', 'N/A')}\")\n",
    "        print(f\"  Learning Rate: {train_args.get('learning_rate', 'N/A')}\")\n",
    "        print(f\"  Dense Architecture: {train_args.get('dense_arch_layer_sizes', 'N/A')}\")\n",
    "        print(f\"  Over Architecture: {train_args.get('over_arch_layer_sizes', 'N/A')}\")\n",
    "        \n",
    "        # Validation progress\n",
    "        print(f\"\\nValidation AUROC Progress:\")\n",
    "        for i, auroc in enumerate(val_aurocs):\n",
    "            epoch_label = \"Initial\" if i == 0 else f\"Epoch {i}\"\n",
    "            print(f\"  {epoch_label}: {auroc:.4f}\")\n",
    "        \n",
    "        # Simple plotting with matplotlib if available\n",
    "        try:\n",
    "            import matplotlib.pyplot as plt\n",
    "            \n",
    "            plt.figure(figsize=(10, 6))\n",
    "            epochs = list(range(len(val_aurocs)))\n",
    "            plt.plot(epochs, val_aurocs, 'b-o', label='Validation AUROC')\n",
    "            plt.axhline(y=test_auroc, color='r', linestyle='--', label=f'Test AUROC ({test_auroc:.4f})')\n",
    "            plt.xlabel('Epoch')\n",
    "            plt.ylabel('AUROC')\n",
    "            plt.title('DLRM Book Recommendation Training Progress')\n",
    "            plt.legend()\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            plt.tight_layout()\n",
    "            plt.savefig('dlrm_training_progress.png', dpi=300, bbox_inches='tight')\n",
    "            plt.show()\n",
    "            print(\"\\n‚úÖ Training progress plot saved as 'dlrm_training_progress.png'\")\n",
    "            \n",
    "        except ImportError:\n",
    "            print(\"\\n‚ö†Ô∏è matplotlib not available - skipping plot generation\")\n",
    "        \n",
    "        return results\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(\"‚ùå Results file not found. Please complete training first.\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error analyzing results: {e}\")\n",
    "        return None\n",
    "\n",
    "print(\"‚úÖ Performance analysis function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 25: Model Comparison Utilities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model comparison utilities defined\n"
     ]
    }
   ],
   "source": [
    "def compare_model_configs():\n",
    "    \"\"\"Compare different model configurations\"\"\"\n",
    "    configs = {\n",
    "        'Small': config_small,\n",
    "        'Medium': config_medium, \n",
    "        'Large': config_large\n",
    "    }\n",
    "    \n",
    "    print(\"üîç Model Configuration Comparison:\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for name, config in configs.items():\n",
    "        print(f\"\\n{name} Configuration:\")\n",
    "        print(f\"  Epochs: {config.epochs}\")\n",
    "        print(f\"  Embedding Dim: {config.embedding_dim}\")\n",
    "        print(f\"  Batch Size: {config.batch_size}\")\n",
    "        print(f\"  Learning Rate: {config.learning_rate}\")\n",
    "        print(f\"  Dense Layers: {config.dense_arch_layer_sizes}\")\n",
    "        print(f\"  Over Layers: {config.over_arch_layer_sizes}\")\n",
    "        \n",
    "        # Estimate parameter count (approximate)\n",
    "        if cat_cols and emb_counts:\n",
    "            emb_params = sum(count * config.embedding_dim for count in emb_counts)\n",
    "            dense_params = 0\n",
    "            \n",
    "            # Dense architecture\n",
    "            prev_size = len(dense_cols)\n",
    "            for size in config.dense_arch_layer_sizes:\n",
    "                dense_params += prev_size * size + size  # weights + bias\n",
    "                prev_size = size\n",
    "            \n",
    "            # Over architecture  \n",
    "            prev_size = prev_size + len(cat_cols) * config.embedding_dim\n",
    "            for size in config.over_arch_layer_sizes:\n",
    "                dense_params += prev_size * size + size\n",
    "                prev_size = size\n",
    "            \n",
    "            total_params = emb_params + dense_params\n",
    "            print(f\"  Est. Parameters: ~{total_params:,}\")\n",
    "            print(f\"    - Embedding: ~{emb_params:,}\")\n",
    "            print(f\"    - Dense: ~{dense_params:,}\")\n",
    "\n",
    "print(\"‚úÖ Model comparison utilities defined\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 26: Advanced Training Options\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Advanced training options defined\n"
     ]
    }
   ],
   "source": [
    "def train_with_early_stopping(\n",
    "    args: BookDLRMArgs,\n",
    "    patience: int = 3,\n",
    "    min_delta: float = 0.001,\n",
    "    restore_best_weights: bool = True\n",
    "):\n",
    "    \"\"\"Enhanced training with early stopping\"\"\"\n",
    "    print(f\"üîÑ Training with early stopping (patience={patience}, min_delta={min_delta})\")\n",
    "    \n",
    "    # This would be integrated into the main training loop\n",
    "    # For now, just show the concept\n",
    "    \n",
    "    class EarlyStopping:\n",
    "        def __init__(self, patience=3, min_delta=0.001):\n",
    "            self.patience = patience\n",
    "            self.min_delta = min_delta\n",
    "            self.best_score = None\n",
    "            self.counter = 0\n",
    "            self.early_stop = False\n",
    "            \n",
    "        def __call__(self, val_score):\n",
    "            if self.best_score is None:\n",
    "                self.best_score = val_score\n",
    "            elif val_score > self.best_score + self.min_delta:\n",
    "                self.best_score = val_score\n",
    "                self.counter = 0\n",
    "            else:\n",
    "                self.counter += 1\n",
    "                if self.counter >= self.patience:\n",
    "                    self.early_stop = True\n",
    "            \n",
    "            return self.early_stop\n",
    "    \n",
    "    early_stopping = EarlyStopping(patience, min_delta)\n",
    "    print(\"‚úÖ Early stopping mechanism ready\")\n",
    "    \n",
    "    return early_stopping\n",
    "\n",
    "def create_learning_rate_schedule(\n",
    "    base_lr: float = 0.01,\n",
    "    schedule_type: str = \"cosine\",\n",
    "    warmup_epochs: int = 1,\n",
    "    total_epochs: int = 10\n",
    "):\n",
    "    \"\"\"Create different learning rate schedules\"\"\"\n",
    "    schedules = {\n",
    "        \"constant\": f\"Constant LR: {base_lr}\",\n",
    "        \"step\": f\"Step decay from {base_lr}\",\n",
    "        \"cosine\": f\"Cosine annealing from {base_lr}\",\n",
    "        \"exponential\": f\"Exponential decay from {base_lr}\"\n",
    "    }\n",
    "    \n",
    "    print(f\"üìà Learning Rate Schedule: {schedules.get(schedule_type, 'Custom')}\")\n",
    "    print(f\"   Base LR: {base_lr}\")\n",
    "    print(f\"   Warmup epochs: {warmup_epochs}\")\n",
    "    print(f\"   Total epochs: {total_epochs}\")\n",
    "    \n",
    "    return schedule_type\n",
    "\n",
    "print(\"‚úÖ Advanced training options defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 27: Data Validation and Debugging\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Training setup validation function defined\n"
     ]
    }
   ],
   "source": [
    "def validate_training_setup():\n",
    "    \"\"\"Validate that all components are ready for training\"\"\"\n",
    "    print(\"üîç Validating Training Setup...\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    checks = []\n",
    "    \n",
    "    # Check preprocessing info\n",
    "    if dense_cols and cat_cols and emb_counts:\n",
    "        checks.append((\"‚úÖ\", \"Preprocessing info loaded\"))\n",
    "        print(f\"   Dense features: {len(dense_cols)}\")\n",
    "        print(f\"   Categorical features: {len(cat_cols)}\")\n",
    "    else:\n",
    "        checks.append((\"‚ùå\", \"Missing preprocessing info\"))\n",
    "    \n",
    "    # Check data directories\n",
    "    data_dirs = [\"dlrm_book_data/mds_train\", \"dlrm_book_data/mds_validation\", \"dlrm_book_data/mds_test\"]\n",
    "    all_dirs_exist = all(os.path.exists(d) for d in data_dirs)\n",
    "    if all_dirs_exist:\n",
    "        checks.append((\"‚úÖ\", \"Data directories found\"))\n",
    "    else:\n",
    "        checks.append((\"‚ùå\", \"Missing data directories\"))\n",
    "        for d in data_dirs:\n",
    "            if not os.path.exists(d):\n",
    "                print(f\"     Missing: {d}\")\n",
    "    \n",
    "    # Check GPU/CPU setup\n",
    "    checks.append((\"‚úÖ\", f\"Device: {device}\"))\n",
    "    \n",
    "    # Check distributed setup\n",
    "    try:\n",
    "        rank = int(os.environ.get(\"RANK\", \"0\"))\n",
    "        world_size = int(os.environ.get(\"WORLD_SIZE\", \"1\"))\n",
    "        checks.append((\"‚úÖ\", f\"Distributed setup: rank {rank}/{world_size}\"))\n",
    "    except:\n",
    "        checks.append((\"‚ö†Ô∏è\", \"Distributed setup incomplete\"))\n",
    "    \n",
    "    # Display results\n",
    "    print(f\"\\nValidation Results:\")\n",
    "    for status, message in checks:\n",
    "        print(f\"{status} {message}\")\n",
    "    \n",
    "    all_good = all(check[0] == \"‚úÖ\" for check in checks)\n",
    "    if all_good:\n",
    "        print(f\"\\nüéØ All systems ready for training!\")\n",
    "    else:\n",
    "        print(f\"\\n‚ö†Ô∏è Please resolve issues before training\")\n",
    "    \n",
    "    return all_good\n",
    "\n",
    "print(\"‚úÖ Training setup validation function defined\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 28: Quick Test Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Quick test functions defined\n"
     ]
    }
   ],
   "source": [
    "def quick_data_test():\n",
    "    \"\"\"Quick test to verify data loading works\"\"\"\n",
    "    try:\n",
    "        print(\"üß™ Quick data loading test...\")\n",
    "        \n",
    "        # Test loading a small batch\n",
    "        test_dataloader = get_dataloader_with_mosaic(\n",
    "            \"dlrm_book_data/mds_train\", \n",
    "            batch_size=32, \n",
    "            label=\"test_batch\"\n",
    "        )\n",
    "        \n",
    "        if test_dataloader:\n",
    "            # Get one batch\n",
    "            batch_iter = iter(test_dataloader)\n",
    "            sample_batch = next(batch_iter)\n",
    "            \n",
    "            print(f\"‚úÖ Successfully loaded test batch\")\n",
    "            print(f\"   Batch keys: {list(sample_batch.keys())}\")\n",
    "            print(f\"   Label shape: {sample_batch['label'].shape if hasattr(sample_batch['label'], 'shape') else 'N/A'}\")\n",
    "            \n",
    "            # Test transformation\n",
    "            if transform_partial:\n",
    "                transformed_batch = transform_partial(sample_batch)\n",
    "                print(f\"‚úÖ Batch transformation successful\")\n",
    "                print(f\"   Dense features shape: {transformed_batch.dense_features.shape}\")\n",
    "                print(f\"   Sparse features keys: {transformed_batch.sparse_features.keys()}\")\n",
    "                print(f\"   Labels shape: {transformed_batch.labels.shape}\")\n",
    "            else:\n",
    "                print(\"‚ùå Transform function not available\")\n",
    "                \n",
    "        else:\n",
    "            print(\"‚ùå Failed to create test dataloader\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Data test failed: {e}\")\n",
    "\n",
    "def quick_model_test():\n",
    "    \"\"\"Quick test to verify model creation works\"\"\"\n",
    "    try:\n",
    "        print(\"üß™ Quick model creation test...\")\n",
    "        \n",
    "        if cat_cols and emb_counts:\n",
    "            # Create minimal model for testing\n",
    "            test_eb_configs = [\n",
    "                EmbeddingBagConfig(\n",
    "                    name=f\"test_{feature_name}\",\n",
    "                    embedding_dim=8,  # Small for testing\n",
    "                    num_embeddings=emb_counts[feature_idx],\n",
    "                    feature_names=[feature_name],\n",
    "                )\n",
    "                for feature_idx, feature_name in enumerate(cat_cols[:2])  # Only first 2 features\n",
    "            ]\n",
    "            \n",
    "            test_dlrm = DLRM(\n",
    "                embedding_bag_collection=EmbeddingBagCollection(\n",
    "                    tables=test_eb_configs, device=device\n",
    "                ),\n",
    "                dense_in_features=len(dense_cols),\n",
    "                dense_arch_layer_sizes=[32, 16],\n",
    "                over_arch_layer_sizes=[64, 32, 1],\n",
    "                dense_device=device,\n",
    "            )\n",
    "            \n",
    "            test_model = DLRMTrain(test_dlrm).to(device)\n",
    "            num_params = sum(p.numel() for p in test_model.parameters())\n",
    "            \n",
    "            print(f\"‚úÖ Test model created successfully\")\n",
    "            print(f\"   Parameters: {num_params:,}\")\n",
    "            print(f\"   Device: {next(test_model.parameters()).device}\")\n",
    "            \n",
    "        else:\n",
    "            print(\"‚ùå Cannot create test model - missing feature info\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Model test failed: {e}\")\n",
    "\n",
    "print(\"‚úÖ Quick test functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 29: Training Progress Monitoring\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Training monitoring setup defined\n"
     ]
    }
   ],
   "source": [
    "def setup_training_monitoring():\n",
    "    \"\"\"Setup training monitoring and logging\"\"\"\n",
    "    \n",
    "    class TrainingMonitor:\n",
    "        def __init__(self):\n",
    "            self.start_time = None\n",
    "            self.epoch_times = []\n",
    "            self.best_val_auroc = 0.0\n",
    "            self.training_history = {\n",
    "                'val_aurocs': [],\n",
    "                'epoch_times': [],\n",
    "                'learning_rates': []\n",
    "            }\n",
    "        \n",
    "        def start_training(self):\n",
    "            import time\n",
    "            self.start_time = time.time()\n",
    "            print(f\"üöÄ Training started at {time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "        \n",
    "        def log_epoch(self, epoch, val_auroc, lr=None):\n",
    "            import time\n",
    "            current_time = time.time()\n",
    "            \n",
    "            if epoch == 0:\n",
    "                epoch_time = 0\n",
    "            else:\n",
    "                epoch_time = current_time - (self.start_time + sum(self.epoch_times))\n",
    "            \n",
    "            self.epoch_times.append(epoch_time)\n",
    "            self.training_history['val_aurocs'].append(val_auroc)\n",
    "            self.training_history['epoch_times'].append(epoch_time)\n",
    "            if lr:\n",
    "                self.training_history['learning_rates'].append(lr)\n",
    "            \n",
    "            if val_auroc > self.best_val_auroc:\n",
    "                self.best_val_auroc = val_auroc\n",
    "                print(f\"üéØ New best validation AUROC: {val_auroc:.4f}\")\n",
    "            \n",
    "            total_time = current_time - self.start_time\n",
    "            avg_epoch_time = total_time / (epoch + 1) if epoch > 0 else 0\n",
    "            \n",
    "            print(f\"üìä Epoch {epoch} Summary:\")\n",
    "            print(f\"   Val AUROC: {val_auroc:.4f}\")\n",
    "            print(f\"   Epoch time: {epoch_time:.1f}s\")\n",
    "            print(f\"   Total time: {total_time:.1f}s\")\n",
    "            print(f\"   Avg epoch time: {avg_epoch_time:.1f}s\")\n",
    "            if lr:\n",
    "                print(f\"   Learning rate: {lr:.6f}\")\n",
    "        \n",
    "        def finish_training(self):\n",
    "            import time\n",
    "            total_time = time.time() - self.start_time\n",
    "            print(f\"üèÅ Training completed!\")\n",
    "            print(f\"   Total time: {total_time:.1f}s ({total_time/60:.1f} minutes)\")\n",
    "            print(f\"   Best validation AUROC: {self.best_val_auroc:.4f}\")\n",
    "            print(f\"   Average epoch time: {np.mean(self.epoch_times):.1f}s\")\n",
    "    \n",
    "    monitor = TrainingMonitor()\n",
    "    print(\"‚úÖ Training monitor setup complete\")\n",
    "    return monitor\n",
    "\n",
    "print(\"‚úÖ Training monitoring setup defined\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
